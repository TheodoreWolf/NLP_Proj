{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sustainable-confusion",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device: NVIDIA GeForce RTX 3070 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "from torchtext.legacy.data import Field, LabelField, TabularDataset, BucketIterator\n",
    "\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Current device:', torch.cuda.get_device_name() if torch.cuda.is_available() else 'CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "657bd36e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 10930, 2023, 2003, 3817, 999, 102]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokeniser test\n",
    "tokenizer.encode('Yo this is Daniel!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adopted-istanbul",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset: transfer to json type so that torchtext reads it\n",
    "def prepare_train_dataset():\n",
    "        \n",
    "    # X = [[sentence1, word1], [sentence2, word2], ...], y = [score1, score2, ...]\n",
    "    X_tr = []\n",
    "    y_tr = []\n",
    "    f = open(\"./datasets/Sub-task 1/lcp_single_train.tsv\")\n",
    "    read_tsv = csv.reader(f, delimiter=\"\\t\")\n",
    "    next(read_tsv, None) # skip header\n",
    "    \n",
    "    train_json = open('./temp/train.json', 'w')\n",
    "    \n",
    "    for data in read_tsv:\n",
    "        d = {'sentence': data[2], 'label': float(data[4])}\n",
    "        json.dump(d, train_json)\n",
    "        train_json.write('\\n')\n",
    "    f.close()\n",
    "\n",
    "prepare_train_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11d53837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special token ids: \n",
      " SOS:101, EOS: 102, PAD: 0, UNK: 100\n",
      "Training terator created with 226 batches.\n"
     ]
    }
   ],
   "source": [
    "# text and label preprocessing - TO FIX\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "sos_idx = tokenizer.cls_token_id\n",
    "eos_idx = tokenizer.sep_token_id\n",
    "pad_idx = tokenizer.pad_token_id\n",
    "unk_idx = tokenizer.unk_token_id\n",
    "\n",
    "print(f'Special token ids: \\n SOS:{sos_idx}, EOS: {eos_idx}, PAD: {pad_idx}, UNK: {unk_idx}')\n",
    "\n",
    "# define fields\n",
    "text_field = Field(use_vocab=True,\n",
    "                   tokenize=tokenizer.encode,\n",
    "#                    preprocessing = tokenizer.convert_tokens_to_ids,\n",
    "                   batch_first=True,\n",
    "                   pad_token=pad_idx,\n",
    "                   unk_token=unk_idx,\n",
    "                   init_token=sos_idx,\n",
    "                   eos_token=eos_idx)\n",
    "\n",
    "label_field = LabelField(use_vocab=False,\n",
    "                    batch_first=True,\n",
    "                    dtype=torch.float)\n",
    "\n",
    "fields = {'sentence': ('sentence', text_field), 'label': ('label',label_field)}\n",
    "\n",
    "# dataloader using json format\n",
    "json_path = \"./temp/\"\n",
    "train_data = TabularDataset(\n",
    "    path =  json_path + 'train.json',\n",
    "    format='json',\n",
    "    fields = fields)\n",
    "\n",
    "text_field.build_vocab(train_data)\n",
    "\n",
    "# iterators\n",
    "train_it = BucketIterator(train_data,\n",
    "                          batch_size=batch_size,\n",
    "                          sort_key= lambda x: len(x.sentence),\n",
    "                          repeat=True,\n",
    "                          sort=False,\n",
    "                          shuffle=True,\n",
    "                          device=device)\n",
    "# valid_it ...\n",
    "# test_it ...\n",
    "\n",
    "print('Training terator created with', len(train_it), 'batches.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7dc76c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########Dataloader test########\n",
      "Tokenised sentence: \n",
      "     [101, 2057, 16012, 15869, 2135, 7594, 1996, 3815, 1997, 9572, 2094, 1037, 29720, 1999, 1996, 14332, 1997, 2256, 12328, 2077, 1998, 2044, 9099, 6914, 2063, 16341, 2478, 11307, 8132, 4106, 1997, 2522, 28228, 9289, 8153, 2013, 2169, 4111, 1012, 102]\n",
      "Original sentence: \n",
      "     ['[CLS]', 'we', 'bio', '##chemical', '##ly', 'measured', 'the', 'amount', 'of', 'aggregate', '##d', 'a', '##β', 'in', 'the', 'brains', 'of', 'our', 'mice', 'before', 'and', 'after', 'trans', '##gen', '##e', 'suppression', 'using', 'filter', 'trap', 'analysis', 'of', 'co', '##rti', '##cal', 'tissue', 'from', 'each', 'animal', '.', '[SEP]']\n",
      "Lexical complexity label:\n",
      "     0.1764705882352941 \n",
      "\n",
      "########Iterator test########\n",
      "Tokenised sentence: \n",
      "     [2, 2, 2942, 5, 4, 1472, 89, 711, 824, 9, 128, 98, 93, 753, 3105, 11, 4456, 573, 1868, 1918, 4400, 1244, 6, 1472, 767, 5, 374, 153, 2487, 9, 4, 216, 4592, 76, 5, 25, 8, 217, 8, 5, 5128, 71, 62, 7, 359, 98, 100, 5, 345, 1472, 89, 1580, 1850, 48, 217, 56, 8, 188, 12, 217, 872, 23, 2963, 1828, 6, 4, 3395, 462, 735, 19, 1314, 110, 20, 5, 7, 296, 5797, 103, 3682, 5, 37, 9, 884, 1383, 1580, 1259, 14, 1472, 767, 3203, 67, 10, 9606, 1527, 4, 149, 828, 2752, 101, 330, 31, 848, 12, 1137, 32, 8, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Original sentence: \n",
      "     ['[unused1]', '[unused1]', 'civil', '[unused4]', '[unused3]', 'ᅥ', '[unused88]', '[unused706]', '[unused819]', '[unused8]', '[unused123]', '[unused97]', '[unused92]', '[unused748]', 'job', '[unused10]', 'cancer', '[unused568]', '昭', '目', 'wave', 'ד', '[unused5]', 'ᅥ', '[unused762]', '[unused4]', '[unused369]', '[unused148]', '##1', '[unused8]', '[unused3]', '[unused211]', 'interior', '[unused75]', '[unused4]', '[unused24]', '[unused7]', '[unused212]', '[unused7]', '[unused4]', 'putting', '[unused70]', '[unused61]', '[unused6]', '[unused354]', '[unused97]', '[UNK]', '[unused4]', '[unused340]', 'ᅥ', '[unused88]', '™', '忄', '[unused47]', '[unused212]', '[unused55]', '[unused7]', '[unused183]', '[unused11]', '[unused212]', '[unused867]', '[unused22]', 'hear', '將', '[unused5]', '[unused3]', 'subject', '[unused457]', '[unused730]', '[unused18]', 'ए', '[unused105]', '[unused19]', '[unused4]', '[unused6]', '[unused291]', 'baron', '[MASK]', 'piano', '[unused4]', '[unused36]', '[unused8]', '[unused879]', 'ச', '™', 'ע', '[unused13]', 'ᅥ', '[unused762]', 'lady', '[unused66]', '[unused9]', 'melissa', '‡', '[unused3]', '[unused144]', '[unused823]', 'areas', '[CLS]', '[unused325]', '[unused30]', '[unused843]', '[unused11]', 'ʑ', '[unused31]', '[unused7]', '[unused2]', '[unused2]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]']\n",
      "Lexical complexity label:\n",
      "     0.25\n"
     ]
    }
   ],
   "source": [
    "# print an example\n",
    "print('########Dataloader test########')\n",
    "random_idx = np.random.randint(0, len(train_data))\n",
    "print('Tokenised sentence: \\n    ', vars(train_data.examples[random_idx])['sentence'])\n",
    "print('Original sentence: \\n    ', tokenizer.convert_ids_to_tokens(vars(train_data.examples[random_idx])['sentence']))\n",
    "print('Lexical complexity label:\\n    ', vars(train_data.examples[random_idx])['label'], '\\n')\n",
    "\n",
    "print('########Iterator test########')\n",
    "for batch in train_it:\n",
    "    sentence, label = batch.sentence[0].cpu().tolist(), batch.label[0].cpu().numpy()\n",
    "    print('Tokenised sentence: \\n    ', sentence)\n",
    "    print('Original sentence: \\n    ', tokenizer.convert_ids_to_tokens(sentence))\n",
    "    print('Lexical complexity label:\\n    ', label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b0b81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model: BERT + biLSTM + MLP\n",
    "class BERT_model(nn.Module):\n",
    "    def __init__(self, bert, hidden_dim, output_dim, layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.bidirectional = bidirectional\n",
    "        emb_dim = bert.config.to_dict()['hidden_size']\n",
    "        \n",
    "        self.lstm = nn.LSTM(emb_dim, \n",
    "                            hidden_dim, \n",
    "                            layers,\n",
    "                            bidirectional=bidirectional,\n",
    "                            batch_first=True,\n",
    "                            dropout = 0 if layers < 2 else dropout)\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            fc_input_dim = hidden_dim * 2\n",
    "        else:\n",
    "            fc_input_dim = hidden_dim\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(fc_input_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.bert(x)[0] # [bs, len, emb_dim]\n",
    "        \n",
    "        _, hn = self.lstm(embeddings) # [l * d, bs, emb_dim]\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            hn = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim=1)\n",
    "        \n",
    "        hn = self.relu(hn)\n",
    "        hn = self.fc(hn)\n",
    "        output = self.sigmoid(hn)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abe9a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 128\n",
    "output_dim = 1\n",
    "layers = 3\n",
    "bidirectional = True\n",
    "dropout = 0.1\n",
    "\n",
    "model = BERT_model(bert=bert,\n",
    "                   hidden_dim=hidden_dim,\n",
    "                   output_dim=output_dim,\n",
    "                   layers=layers,\n",
    "                   bidirectional=bidirectional,\n",
    "                   dropout=dropout)\n",
    "model.to(device)\n",
    "\n",
    "# freeze bert params\n",
    "for name, param in model.named_parameters():                \n",
    "    if name.startswith('bert'):\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "print(f'The model has {sum(p.numel() for p in model.parameters()):,} total parameters')\n",
    "print(f'The model has {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f12d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.MSELoss()\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bcbcb7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train function\n",
    "def train(iterator, model, optimiser, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer = zero_grad()\n",
    "        pred = model(batch.sentence)\n",
    "        loss = criterion(pre, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    return eposs_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f9f1065b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation function\n",
    "def eval(iterator, model, criterion):\n",
    "    model.eval()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee21c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main loop\n",
    "n_epochs = 10\n",
    "start = time.time()\n",
    "\n",
    "for epoch in tqdm(n_epochs):\n",
    "    \n",
    "    epoch_start = time.time()\n",
    "    train_loss = train(train_it, model, optimiser, criterion)\n",
    "    # valid_loss ...\n",
    "    \n",
    "    epoch_end = time.time()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
