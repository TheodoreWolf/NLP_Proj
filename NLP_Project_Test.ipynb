{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "sustainable-confusion",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokeniser test: [101, 10930, 2023, 2003, 3817, 999, 102]\n",
      "Current device: NVIDIA GeForce RTX 3070 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "from torchtext.legacy.data import Field, LabelField, TabularDataset, BucketIterator\n",
    "\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "# tokeniser test\n",
    "print('Tokeniser test:', tokenizer.encode('Yo this is Daniel!'))\n",
    "\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Current device:', torch.cuda.get_device_name() if torch.cuda.is_available() else 'CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adopted-istanbul",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7662 training entries and 917 test entreies.\n"
     ]
    }
   ],
   "source": [
    "# load datasets\n",
    "src_path = './datasets/'\n",
    "train_file = 'lcp_single_train.tsv.txt'\n",
    "test_file = 'lcp_single_test_labels.tsv.txt'\n",
    "temp_path = './temp/'\n",
    "\n",
    "load_dataset_as_json(src_path, train_file, test_file, temp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "11d53837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special token ids: \n",
      " SOS:101, EOS: 102, PAD: 0, UNK: 100\n",
      "Training iterator created with 240 batches.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sentence': [100,\n",
       "  100,\n",
       "  100,\n",
       "  100,\n",
       "  100,\n",
       "  100,\n",
       "  100,\n",
       "  100,\n",
       "  100,\n",
       "  100,\n",
       "  100,\n",
       "  100,\n",
       "  100,\n",
       "  100,\n",
       "  100,\n",
       "  100,\n",
       "  100,\n",
       "  100,\n",
       "  100,\n",
       "  100,\n",
       "  100,\n",
       "  100,\n",
       "  100,\n",
       "  100,\n",
       "  100,\n",
       "  100],\n",
       " 'complexity': 0.0}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocessing\n",
    "batch_size = 32\n",
    "max_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
    "\n",
    "sos_idx = tokenizer.cls_token_id\n",
    "eos_idx = tokenizer.sep_token_id\n",
    "pad_idx = tokenizer.pad_token_id\n",
    "unk_idx = tokenizer.unk_token_id\n",
    "\n",
    "print(f'Special token ids: \\n SOS:{sos_idx}, EOS: {eos_idx}, PAD: {pad_idx}, UNK: {unk_idx}')\n",
    "\n",
    "# define fields\n",
    "# text_field = Field(sequential=False,\n",
    "#                    use_vocab=True,\n",
    "#                    init_token=sos_idx,\n",
    "#                    eos_token=eos_idx,\n",
    "#                    tokenize=tokenizer.encode,\n",
    "# #                    preprocessing = tokenizer.convert_tokens_to_ids,\n",
    "#                    lower=False,\n",
    "#                    batch_first=True,\n",
    "#                    pad_token=pad_idx,\n",
    "#                    unk_token=unk_idx,\n",
    "#                    is_target=False)\n",
    "\n",
    "text_field = Field(use_vocab=True,\n",
    "                   tokenize=tokenizer.encode,\n",
    "#                    preprocessing = tokenizer.convert_tokens_to_ids,\n",
    "                   batch_first=True,\n",
    "                   pad_token=pad_idx,\n",
    "                   unk_token=unk_idx,\n",
    "                   init_token=sos_idx,\n",
    "                   eos_token=eos_idx)\n",
    "\n",
    "label_field = LabelField(use_vocab=False,\n",
    "                         batch_first=True,\n",
    "                         is_target=True,\n",
    "                         dtype=torch.float)\n",
    "\n",
    "fields = {'sentence': ('sentence', text_field), 'complexity': ('complexity', label_field)}\n",
    "\n",
    "# dataloader using json format\n",
    "train_data = TabularDataset(path = temp_path + 'train.json', format='json', fields=fields)\n",
    "test_data = TabularDataset(path = temp_path + 'test.json', format='json', fields=fields)\n",
    "\n",
    "text_field.build_vocab(train_data)\n",
    "\n",
    "# iterators\n",
    "train_it = BucketIterator(train_data,\n",
    "                          batch_size=batch_size,\n",
    "                          sort_key= None,\n",
    "                          repeat=True,\n",
    "                          sort=False,\n",
    "                          shuffle=True,\n",
    "                          device=device)\n",
    "# valid_it ...\n",
    "# test_it ...\n",
    "\n",
    "print('Training iterator created with', len(train_it), 'batches.')\n",
    "vars(train_data.examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "62344cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7632, 1045, 2572]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(tokenizer.tokenize('hi I am'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7dc76c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########Dataloader test########\n",
      "Tokenised sentence: \n",
      "     [101, 5678, 1010, 2048, 5022, 2007, 3972, 20624, 5644, 4593, 19129, 1996, 9666, 2475, 25206, 2031, 2351, 2013, 3674, 27480, 2019, 9626, 11983, 2164, 3729, 2232, 1031, 4229, 1522, 9932, 12740, 1033, 1012, 102]\n",
      "Original sentence: \n",
      "     ['[CLS]', 'additionally', ',', 'two', 'patients', 'with', 'del', '##eti', '##ons', 'apparently', 'encompassing', 'the', 'fog', '##2', 'locus', 'have', 'died', 'from', 'multiple', 'congenital', 'an', '##oma', '##lies', 'including', 'cd', '##h', '[', '38', '‚', 'ai', '##40', ']', '.', '[SEP]']\n",
      "Lexical complexity label:\n",
      "     0.5277777777777778 \n",
      "\n",
      "########Iterator test########\n",
      "Tokenised sentence: \n",
      "     [2, 2, 467, 440, 17, 4, 546, 694, 1664, 1025, 1119, 191, 14, 555, 12, 922, 2357, 61, 36, 652, 5, 4, 3426, 8262, 1341, 13, 108, 153, 12, 622, 1467, 6, 171, 506, 2357, 61, 36, 652, 2088, 1562, 10, 5745, 1047, 6, 11, 439, 243, 8, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Original sentence: \n",
      "     ['[unused1]', '[unused1]', '[unused462]', '[unused435]', '[unused16]', '[unused3]', '[unused541]', '[unused689]', 'つ', ';', 'ɪ', '[unused186]', '[unused13]', '[unused550]', '[unused11]', '[unused917]', 'turned', '[unused60]', '[unused35]', '[unused647]', '[unused4]', '[unused3]', 'cause', 'roland', 'ि', '[unused12]', '[unused103]', '[unused148]', '[unused11]', '[unused617]', 'ᄐ', '[unused5]', '[unused166]', '[unused501]', 'turned', '[unused60]', '[unused35]', '[unused647]', 'world', 'ₒ', '[unused9]', 'ghost', 'k', '[unused5]', '[unused10]', '[unused434]', '[unused238]', '[unused7]', '[unused2]', '[unused2]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]']\n",
      "Lexical complexity label:\n",
      "     0.3382353\n"
     ]
    }
   ],
   "source": [
    "# print an example\n",
    "print('########Dataloader test########')\n",
    "random_idx = np.random.randint(0, len(train_data))\n",
    "print('Tokenised sentence: \\n    ', vars(train_data.examples[random_idx])['sentence'])\n",
    "print('Original sentence: \\n    ', tokenizer.convert_ids_to_tokens(vars(train_data.examples[random_idx])['sentence']))\n",
    "print('Lexical complexity label:\\n    ', vars(train_data.examples[random_idx])['complexity'], '\\n')\n",
    "\n",
    "print('########Iterator test########')\n",
    "for batch in train_it:\n",
    "    sentence, complexity = batch.sentence[0].cpu().tolist(), batch.complexity[0].cpu().numpy()\n",
    "    print('Tokenised sentence: \\n    ', sentence)\n",
    "    print('Original sentence: \\n    ', tokenizer.convert_ids_to_tokens(sentence))\n",
    "    print('Lexical complexity label:\\n    ', complexity)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b0b81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model: BERT + biLSTM + MLP\n",
    "class BERT_model(nn.Module):\n",
    "    def __init__(self, bert, hidden_dim, output_dim, layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.bidirectional = bidirectional\n",
    "        emb_dim = bert.config.to_dict()['hidden_size']\n",
    "        \n",
    "        self.lstm = nn.LSTM(emb_dim, \n",
    "                            hidden_dim, \n",
    "                            layers,\n",
    "                            bidirectional=bidirectional,\n",
    "                            batch_first=True,\n",
    "                            dropout = 0 if layers < 2 else dropout)\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            fc_input_dim = hidden_dim * 2\n",
    "        else:\n",
    "            fc_input_dim = hidden_dim\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(fc_input_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.bert(x)[0] # [bs, len, emb_dim]\n",
    "        \n",
    "        _, hn = self.lstm(embeddings) # [l * d, bs, emb_dim]\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            hn = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim=1)\n",
    "        \n",
    "        hn = self.relu(hn)\n",
    "        hn = self.fc(hn)\n",
    "        output = self.sigmoid(hn)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abe9a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 128\n",
    "output_dim = 1\n",
    "layers = 3\n",
    "bidirectional = True\n",
    "dropout = 0.1\n",
    "\n",
    "model = BERT_model(bert=bert,\n",
    "                   hidden_dim=hidden_dim,\n",
    "                   output_dim=output_dim,\n",
    "                   layers=layers,\n",
    "                   bidirectional=bidirectional,\n",
    "                   dropout=dropout)\n",
    "model.to(device)\n",
    "\n",
    "# freeze bert params\n",
    "for name, param in model.named_parameters():                \n",
    "    if name.startswith('bert'):\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "print(f'The model has {sum(p.numel() for p in model.parameters()):,} total parameters')\n",
    "print(f'The model has {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f12d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.MSELoss()\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbcb7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train function\n",
    "def train(iterator, model, optimiser, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer = zero_grad()\n",
    "        pred = model(batch.sentence)\n",
    "        loss = criterion(pre, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    return eposs_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f1065b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation function\n",
    "def eval(iterator, model, criterion):\n",
    "    model.eval()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee21c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main loop\n",
    "n_epochs = 10\n",
    "start = time.time()\n",
    "\n",
    "for epoch in tqdm(n_epochs):\n",
    "    \n",
    "    epoch_start = time.time()\n",
    "    train_loss = train(train_it, model, optimiser, criterion)\n",
    "    # valid_loss ...\n",
    "    \n",
    "    epoch_end = time.time()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
