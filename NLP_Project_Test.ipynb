{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sustainable-confusion",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device: NVIDIA GeForce RTX 3070 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "from torchtext.legacy.data import Field, LabelField, TabularDataset, BucketIterator\n",
    "\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Current device:', torch.cuda.get_device_name() if torch.cuda.is_available() else 'CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "657bd36e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 10930, 2023, 2003, 3817, 999, 102]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokeniser test\n",
    "tokenizer.encode('Yo this is Daniel!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adopted-istanbul",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset: transfer to json type so that torchtext reads it\n",
    "def prepare_train_dataset():\n",
    "        \n",
    "    # X = [[sentence1, word1], [sentence2, word2], ...], y = [score1, score2, ...]\n",
    "    X_tr = []\n",
    "    y_tr = []\n",
    "    f = open(\"./datasets/Sub-task 1/lcp_single_train.tsv\")\n",
    "    read_tsv = csv.reader(f, delimiter=\"\\t\")\n",
    "    next(read_tsv, None) # skip header\n",
    "    \n",
    "    train_json = open('./temp/train.json', 'w')\n",
    "    \n",
    "    for data in read_tsv:\n",
    "        d = {'sentence': data[2], 'label': float(data[4])}\n",
    "        json.dump(d, train_json)\n",
    "        train_json.write('\\n')\n",
    "    f.close()\n",
    "\n",
    "prepare_train_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d53837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special token ids: \n",
      " SOS:101, EOS: 102, PAD: 0, UNK: 100\n"
     ]
    }
   ],
   "source": [
    "# text and label preprocessing - TO FIX\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "sos_idx = tokenizer.cls_token_id\n",
    "eos_idx = tokenizer.sep_token_id\n",
    "pad_idx = tokenizer.pad_token_id\n",
    "unk_idx = tokenizer.unk_token_id\n",
    "\n",
    "print(f'Special token ids: \\n SOS:{sos_idx}, EOS: {eos_idx}, PAD: {pad_idx}, UNK: {unk_idx}')\n",
    "\n",
    "# define fields\n",
    "text_field = Field(use_vocab=True,\n",
    "                   tokenize=tokenizer.encode,\n",
    "                   preprocessing = tokenizer.convert_tokens_to_ids,\n",
    "                   batch_first=True,\n",
    "                   pad_token=pad_idx,\n",
    "                   unk_token=unk_idx,\n",
    "                   init_token=sos_idx,\n",
    "                   eos_token=eos_idx)\n",
    "\n",
    "label_field = LabelField(use_vocab=False,\n",
    "                    batch_first=True,\n",
    "                    dtype=torch.float)\n",
    "\n",
    "fields = {'sentence': ('sentence', text_field), 'label': ('label',label_field)}\n",
    "\n",
    "# dataloader using json format\n",
    "json_path = \"./temp/\"\n",
    "train_data = TabularDataset(\n",
    "    path =  json_path + 'train.json',\n",
    "    format='json',\n",
    "    fields = fields)\n",
    "\n",
    "text_field.build_vocab(train_data)\n",
    "\n",
    "# iterators\n",
    "train_it = BucketIterator(train_data,\n",
    "                          batch_size=batch_size,\n",
    "                          sort_key= lambda x: len(x.sentence),\n",
    "                          repeat=True,\n",
    "                          sort=False,\n",
    "                          shuffle=True,\n",
    "                          device=device)\n",
    "# valid_it ...\n",
    "# test_it ...\n",
    "\n",
    "print('Training terator created with', len(train_it), 'batches.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6da2cbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "['[unused1]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[unused2]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]', '[unused0]']\n",
      "tensor([0.4706, 0.5441, 0.2353, 0.0250, 0.3438, 0.4605, 0.3472, 0.6000, 0.4028,\n",
      "        0.3000, 0.4167, 0.2794, 0.1607, 0.0000, 0.2237, 0.3125, 0.3152, 0.2250,\n",
      "        0.5735, 0.3906, 0.4265, 0.1912, 0.2969, 0.4821, 0.4000, 0.1667, 0.2941,\n",
      "        0.2500, 0.3393, 0.2059, 0.3889, 0.4833], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# print an example\n",
    "for batch in train_it:\n",
    "    print(batch.sentence[0])\n",
    "    print(tokenizer.convert_ids_to_tokens(batch.sentence[0]))\n",
    "    print(batch.label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b0b81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model: BERT + biLSTM + MLP\n",
    "class BERT_model(nn.Module):\n",
    "    def __init__(self, bert, hidden_dim, output_dim, layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.bidirectional = bidirectional\n",
    "        emb_dim = bert.config.to_dict()['hidden_size']\n",
    "        \n",
    "        self.lstm = nn.LSTM(emb_dim, \n",
    "                            hidden_dim, \n",
    "                            layers,\n",
    "                            bidirectional=bidirectional,\n",
    "                            batch_first=True,\n",
    "                            dropout = 0 if layers < 2 else dropout)\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            fc_input_dim = hidden_dim * 2\n",
    "        else:\n",
    "            fc_input_dim = hidden_dim\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(fc_input_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.bert(x)[0] # [bs, len, emb_dim]\n",
    "        \n",
    "        _, hn = self.lstm(embeddings) # [l * d, bs, emb_dim]\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            hn = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim=1)\n",
    "        \n",
    "        hn = self.relu(hn)\n",
    "        hn = self.fc(hn)\n",
    "        output = self.sigmoid(hn)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abe9a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 128\n",
    "output_dim = 1\n",
    "layers = 3\n",
    "bidirectional = True\n",
    "dropout = 0.1\n",
    "\n",
    "model = BERT_model(bert=bert,\n",
    "                   hidden_dim=hidden_dim,\n",
    "                   output_dim=output_dim,\n",
    "                   layers=layers,\n",
    "                   bidirectional=bidirectional,\n",
    "                   dropout=dropout)\n",
    "model.to(device)\n",
    "\n",
    "# freeze bert params\n",
    "for name, param in model.named_parameters():                \n",
    "    if name.startswith('bert'):\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "print(f'The model has {sum(p.numel() for p in model.parameters()):,} total parameters')\n",
    "print(f'The model has {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f12d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.MSELoss()\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bcbcb7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train function\n",
    "def train(iterator, model, optimiser, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer = zero_grad()\n",
    "        pred = model(batch.sentence)\n",
    "        loss = criterion(pre, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    return eposs_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f9f1065b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation function\n",
    "def eval(iterator, model, criterion):\n",
    "    model.eval()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee21c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main loop\n",
    "n_epochs = 10\n",
    "start = time.time()\n",
    "\n",
    "for epoch in tqdm(n_epochs):\n",
    "    \n",
    "    epoch_start = time.time()\n",
    "    train_loss = train(train_it, model, optimiser, criterion)\n",
    "    # valid_loss ...\n",
    "    \n",
    "    epoch_end = time.time()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
